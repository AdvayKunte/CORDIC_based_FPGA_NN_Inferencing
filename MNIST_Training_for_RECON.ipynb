{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdvayKunte/CORDIC_based_FPGA_NN_Inferencing/blob/main/MNIST_Training_for_RECON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST Training"
      ],
      "metadata": {
        "id": "N4yoQBtQnV4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary libraries"
      ],
      "metadata": {
        "id": "B0rCblaIndGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2w2L9bJKGfP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dataset"
      ],
      "metadata": {
        "id": "aNZNYT4kngxk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1QEA2xVKZno"
      },
      "outputs": [],
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Casting unit8 to float32"
      ],
      "metadata": {
        "id": "jUYlnOsenkkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7i6RTZKKirp"
      },
      "outputs": [],
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(128)\n",
        "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H7WTxJK-KsF5"
      },
      "outputs": [],
      "source": [
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(128)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining model params and training"
      ],
      "metadata": {
        "id": "Ecin4LF3nokO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw6bJzboKwGq",
        "outputId": "dd19ad23-05fe-4ad0-83ce-c3b41b9c4fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - loss: 1.2024 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.3651 - val_sparse_categorical_accuracy: 0.9077\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.3354 - sparse_categorical_accuracy: 0.9123 - val_loss: 0.2700 - val_sparse_categorical_accuracy: 0.9245\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.2666 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.2304 - val_sparse_categorical_accuracy: 0.9326\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.2218 - sparse_categorical_accuracy: 0.9382 - val_loss: 0.2048 - val_sparse_categorical_accuracy: 0.9400\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1914 - sparse_categorical_accuracy: 0.9455 - val_loss: 0.1852 - val_sparse_categorical_accuracy: 0.9466\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.1745 - sparse_categorical_accuracy: 0.9499 - val_loss: 0.1681 - val_sparse_categorical_accuracy: 0.9509\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1554 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.1564 - val_sparse_categorical_accuracy: 0.9535\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1408 - sparse_categorical_accuracy: 0.9604 - val_loss: 0.1466 - val_sparse_categorical_accuracy: 0.9568\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1296 - sparse_categorical_accuracy: 0.9630 - val_loss: 0.1402 - val_sparse_categorical_accuracy: 0.9571\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1171 - sparse_categorical_accuracy: 0.9673 - val_loss: 0.1299 - val_sparse_categorical_accuracy: 0.9616\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f7d195051d0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "  tf.keras.layers.Dense(10 , activation='sigmoid')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    epochs=10,\n",
        "    validation_data=ds_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint"
      ],
      "metadata": {
        "id": "AlXM445_nufk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3F5ziWTMn8x",
        "outputId": "560db5b8-e8a9-4c0e-bac9-71109d864f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0: flatten\n",
            "  No weights (probably a Flatten layer)\n",
            "Layer 1: dense\n",
            "  Weights shape: (784, 64)\n",
            "  Biases shape: (64,)\n",
            "Layer 2: dense_1\n",
            "  Weights shape: (64, 10)\n",
            "  Biases shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    print(f\"Layer {i}: {layer.name}\")\n",
        "\n",
        "    if weights:\n",
        "        W, b = weights  # W: weights matrix, b: bias vector\n",
        "        print(f\"  Weights shape: {W.shape}\")\n",
        "        print(f\"  Biases shape: {b.shape}\")\n",
        "    else:\n",
        "        print(\"  No weights (probably a Flatten layer)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling the current WnB between -1 to 1 for CORDIC compatibility"
      ],
      "metadata": {
        "id": "YijGwfS_nwhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UumflN_ZOdCw",
        "outputId": "6274cdcf-b184-4a1d-bfa8-d2e103645b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled Layer 1 (dense) weights and biases into [-1, 1]\n",
            "Scaled Layer 2 (dense_1) weights and biases into [-1, 1]\n",
            "‚úÖ Scaled weights saved to 'scaled_weights.npz'\n",
            "\u001b[1m79/79\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2855 - sparse_categorical_accuracy: 0.9393\n",
            "\n",
            "üîç Accuracy after proportional scaling: 93.75%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Scale weights and biases proportionally to fit within [-1, 1]\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if weights:  # Only Dense layers have weights\n",
        "        W, b = weights\n",
        "        W_max = np.max(np.abs(W))\n",
        "        b_max = np.max(np.abs(b))\n",
        "\n",
        "        W_scaled = W  if W_max != 0 else W\n",
        "        b_scaled = b if b_max != 0 else b\n",
        "\n",
        "        layer.set_weights([W_scaled, b_scaled])\n",
        "        print(f\"Scaled Layer {i} ({layer.name}) weights and biases into [-1, 1]\")\n",
        "\n",
        "#Saving scaled weights and biases to a file\n",
        "weights_dict = {}\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if weights:\n",
        "        W, b = weights\n",
        "        weights_dict[f\"layer_{i}_weights\"] = W\n",
        "        weights_dict[f\"layer_{i}_biases\"] = b\n",
        "\n",
        "np.savez(\"scaled_weights.npz\", **weights_dict)\n",
        "print(\"‚úÖ Scaled weights saved to 'scaled_weights.npz'\")\n",
        "\n",
        "#Evaluate the model again to check any significant drops in accuracy\n",
        "loss, accuracy = model.evaluate(ds_test)\n",
        "print(f\"\\nüîç Accuracy after proportional scaling: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the float formatting and keeping only 4 decimal places"
      ],
      "metadata": {
        "id": "wrMT_vFkn_rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMfVRuKgR-2W",
        "outputId": "312697f9-1b0e-4498-850f-f6147697db1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì¶ Layer 0 - flatten_26\n",
            "  üö´ No weights or biases (probably Flatten or non-trainable)\n",
            "\n",
            "üì¶ Layer 1 - dense_52\n",
            "  üßÆ Weights (shape (784, 64)):\n",
            "[[ 0.01    0.0526 -0.052  ... -0.0619  0.0209 -0.0325]\n",
            " [-0.0067 -0.0657 -0.0454 ... -0.0385  0.0505  0.0351]\n",
            " [ 0.0698  0.0105 -0.0556 ... -0.0096  0.0026  0.0008]\n",
            " ...\n",
            " [-0.0638 -0.0602  0.0699 ...  0.0195 -0.0407  0.0688]\n",
            " [-0.0494  0.0241 -0.0479 ... -0.0115  0.0587 -0.0638]\n",
            " [-0.0529  0.0064  0.026  ...  0.0231  0.0349  0.001 ]]\n",
            "  üéØ Biases (shape (64,)):\n",
            "[-0.0784  0.5846  0.0934 -0.2774 -0.0188 -0.5927  0.0186 -0.3744 -0.1269\n",
            " -0.6147  0.4202  0.8733 -0.108  -0.4086  0.8722  0.8987 -0.5914  0.0094\n",
            " -0.5138 -0.1655  0.3967  0.1571 -0.4516  0.3727  1.      0.38    0.8993\n",
            "  0.5807 -0.0974  0.0509  0.4611 -0.1115  0.9262  0.3609  0.7763 -0.5156\n",
            "  0.1357  0.2231  0.1356  0.7859 -0.6142 -0.4101  0.0506 -0.3718 -0.3997\n",
            " -0.7117  0.7448 -0.5013  0.6166 -0.0689 -0.4776  0.4141  0.1494 -0.5433\n",
            "  0.3707  0.2055 -0.5604 -0.5302  0.4937  0.1055 -0.6592 -0.8367 -0.2238\n",
            " -0.3426]\n",
            "\n",
            "üì¶ Layer 2 - dense_53\n",
            "  üßÆ Weights (shape (64, 10)):\n",
            "[[ 0.3236 -0.3942 -0.2269 -0.4694  0.1744  0.1159 -0.3344 -0.3902  0.2176\n",
            "   0.3193]\n",
            " [ 0.2697  0.199  -0.3911 -0.4968  0.1024  0.564  -0.1466  0.3033 -0.8429\n",
            "   0.376 ]\n",
            " [ 0.1102 -0.3764  0.403  -0.3427 -0.4206  0.3118  0.2339 -0.4174  0.1344\n",
            "  -0.3685]\n",
            " [-0.3474 -0.4455  0.2748  0.2868  0.2107 -0.0417 -0.4659  0.2181  0.2928\n",
            "   0.1718]\n",
            " [-0.283  -0.4073 -0.3392 -0.4736  0.415   0.2199  0.1842 -0.0843  0.2442\n",
            "  -0.2291]\n",
            " [-0.3469  0.5     0.1412  0.3855  0.0489 -1.     -0.7277 -0.2196  0.0558\n",
            "   0.0863]\n",
            " [ 0.1899 -0.3505  0.5026 -0.1663 -0.523   0.248  -0.1758 -0.0293  0.1979\n",
            "  -0.0785]\n",
            " [ 0.4234 -0.2054 -0.1728 -0.3436  0.2991 -0.1562  0.3482 -0.3364  0.1867\n",
            "  -0.0502]\n",
            " [ 0.1072  0.1191 -0.23   -0.0414 -0.345  -0.4299  0.1885  0.4459  0.2944\n",
            "  -0.3333]\n",
            " [-0.1349 -0.1855  0.1514  0.098  -0.3406  0.2706 -0.2402 -0.477   0.3912\n",
            "  -0.3597]\n",
            " [ 0.2869 -0.0777  0.3427  0.3669 -0.757   0.4431 -0.3171  0.2489 -0.741\n",
            "  -0.3661]\n",
            " [ 0.3413  0.45    0.4742 -0.1226 -0.0031 -0.0973 -0.0968  0.5353 -0.7118\n",
            "  -0.3335]\n",
            " [ 0.2667  0.1448 -0.3871  0.3879 -0.2342  0.4632 -0.338  -0.5708 -0.5742\n",
            "   0.1755]\n",
            " [ 0.2277  0.2609  0.0153 -0.4659 -0.3738 -0.1188  0.3707 -0.3562  0.2535\n",
            "  -0.3331]\n",
            " [-0.3957 -0.1987  0.4987 -0.3474  0.3771  0.0481  0.3044  0.3518 -0.4342\n",
            "   0.1088]\n",
            " [-0.4677  0.0165 -0.467  -0.17    0.1911  0.4551 -0.205   0.0302 -0.0209\n",
            "   0.2581]\n",
            " [-0.1891  0.1427 -0.3503  0.0248 -0.0651 -0.0968 -0.43   -0.231   0.3131\n",
            "   0.395 ]\n",
            " [-0.4449  0.1184  0.1134 -0.0596  0.4151 -0.0911  0.2759 -0.21    0.1201\n",
            "  -0.6854]\n",
            " [-0.5279 -0.214   0.4531  0.3492  0.136  -0.2439 -0.0015 -0.375  -0.0405\n",
            "  -0.2483]\n",
            " [ 0.269  -0.5061 -0.2918 -0.3355  0.0781  0.0669 -0.2854 -0.1752  0.2231\n",
            "   0.3805]\n",
            " [-0.4761 -0.1913 -0.104   0.066   0.3841  0.2688 -0.4133  0.3896  0.0388\n",
            "  -0.3812]\n",
            " [ 0.2711  0.0337  0.3801 -0.3651  0.0415  0.2273  0.2599 -0.6046 -0.0126\n",
            "  -0.452 ]\n",
            " [ 0.2701  0.3809 -0.2031 -0.3295  0.0242 -0.0205  0.1697 -0.4769  0.2953\n",
            "  -0.3632]\n",
            " [-0.185   0.2435 -0.6245 -0.3228  0.3897  0.3243 -0.3544  0.4359  0.1436\n",
            "  -0.1989]\n",
            " [ 0.0781 -0.126  -0.0469 -0.1401  0.2934  0.5236 -0.0371 -0.0065 -0.262\n",
            "  -0.5025]\n",
            " [-0.1972  0.4003  0.0461 -0.3042 -0.1502 -0.2101 -0.2358  0.3186 -0.4176\n",
            "   0.3738]\n",
            " [-0.0068  0.4681  0.4152 -0.1775  0.3762  0.3555  0.2631 -0.2582 -0.7117\n",
            "  -0.3881]\n",
            " [-0.5994  0.4112  0.4072 -0.4966 -0.46    0.375   0.1069  0.1323 -0.3545\n",
            "  -0.4627]\n",
            " [ 0.3456  0.4005 -0.6451  0.3112 -0.2551  0.2228 -0.0534 -0.6844 -0.0644\n",
            "   0.261 ]\n",
            " [-0.3317  0.2699 -0.0489 -0.4006  0.23   -0.0259  0.0034 -0.1055  0.3256\n",
            "   0.0835]\n",
            " [ 0.1941 -0.1237 -0.6329  0.13    0.2664  0.1028  0.3574  0.6402 -0.527\n",
            "  -0.4761]\n",
            " [-0.4064 -0.2393 -0.071   0.117  -0.1291  0.3166 -0.552   0.204   0.2473\n",
            "   0.1889]\n",
            " [-0.6557  0.3207 -0.2703  0.0536  0.2174  0.343  -0.4628  0.2073 -0.1906\n",
            "   0.2721]\n",
            " [-0.2744  0.2523  0.2055  0.2202 -0.2914 -0.6682 -0.2028  0.3939 -0.4812\n",
            "   0.3579]\n",
            " [-0.0872  0.1867 -0.032  -0.4536  0.1527 -0.3637  0.298   0.4831 -0.588\n",
            "   0.1797]\n",
            " [-0.5515  0.1175 -0.0423  0.3859 -0.3132 -0.2006  0.3713 -0.0068  0.1534\n",
            "  -0.3227]\n",
            " [ 0.3536 -0.2075 -0.4277  0.0043 -0.2059 -0.0912  0.0592  0.1713 -0.5527\n",
            "   0.5056]\n",
            " [ 0.3781  0.1331 -0.1768  0.0846 -0.3032  0.0189 -0.4416  0.5559  0.0847\n",
            "  -0.5091]\n",
            " [ 0.2335 -0.2849  0.2613 -0.0107 -0.4342  0.1928  0.3599 -0.5847 -0.3057\n",
            "   0.1433]\n",
            " [-0.2173  0.4102 -0.0491 -0.4698 -0.532   0.2337  0.4959 -0.1391 -0.0914\n",
            "  -0.4489]\n",
            " [ 0.2743 -0.2725  0.1044  0.2609 -0.5037  0.065  -0.5447  0.1538  0.1102\n",
            "  -0.0764]\n",
            " [-0.4802  0.3129  0.3721  0.4043 -0.1339 -0.3897  0.0055 -0.0181 -0.0133\n",
            "  -0.0828]\n",
            " [ 0.3267 -0.2772  0.3     0.1428 -0.0265  0.3341 -0.2958 -0.227  -0.4305\n",
            "  -0.3497]\n",
            " [ 0.1937 -0.124   0.3775  0.4383  0.2722 -0.6343 -0.3721  0.0997 -0.2447\n",
            "  -0.1522]\n",
            " [ 0.1481  0.1948  0.2064 -0.0455 -0.6859  0.0554 -0.5372 -0.0582  0.1299\n",
            "   0.4498]\n",
            " [ 0.1363  0.1203 -0.0496  0.2636 -0.5499 -0.3558  0.4481 -0.0093  0.2738\n",
            "  -0.4027]\n",
            " [-0.3463  0.3508 -0.3105  0.3191 -0.0549 -0.0547 -0.2696  0.4745 -0.4216\n",
            "  -0.0687]\n",
            " [-0.2103 -0.3308  0.4728 -0.2082 -0.0445 -0.5426 -0.1954  0.2002  0.0041\n",
            "   0.2978]\n",
            " [-0.5377  0.2032 -0.4271  0.3639 -0.2133  0.1873 -0.4658  0.4937  0.0906\n",
            "  -0.0999]\n",
            " [-0.0731  0.2068  0.0528  0.0049 -0.5136  0.3596 -0.234  -0.5779  0.0749\n",
            "  -0.0495]\n",
            " [-0.2092  0.0751  0.2473 -0.1418  0.412  -0.6357 -0.4066 -0.2047  0.0456\n",
            "   0.2966]\n",
            " [-0.1873  0.2167  0.6335  0.0583 -0.4269  0.5572 -0.3941 -0.1903 -0.7419\n",
            "  -0.4575]\n",
            " [-0.1602  0.3839 -0.3645 -0.2814  0.1415 -0.1881  0.3798 -0.4556 -0.0026\n",
            "   0.3676]\n",
            " [ 0.2549 -0.6077 -0.1058  0.2556 -0.3298  0.2148 -0.4269  0.1868  0.1807\n",
            "   0.347 ]\n",
            " [-0.4305 -0.1834  0.1385 -0.3401  0.2616  0.394   0.1939 -0.1977  0.2692\n",
            "  -0.1934]\n",
            " [ 0.0731 -0.4267 -0.3264  0.3007  0.3544  0.0619  0.1823 -0.2335 -0.3878\n",
            "   0.5126]\n",
            " [-0.3171 -0.3851  0.3619  0.3632  0.4278 -0.2937  0.156  -0.2326 -0.1971\n",
            "   0.2728]\n",
            " [ 0.3931 -0.4902 -0.0133  0.0179  0.0246 -0.5608  0.2358  0.3458  0.2591\n",
            "   0.021 ]\n",
            " [-0.2647  0.2317 -0.0062  0.4235  0.4847  0.1194 -0.4036 -0.0846 -0.3779\n",
            "  -0.6215]\n",
            " [ 0.1009  0.0452 -0.4511  0.272  -0.2518 -0.0247  0.2496  0.5287 -0.4335\n",
            "  -0.2509]\n",
            " [-0.2467 -0.4641 -0.575   0.3769  0.0505  0.0252  0.1265 -0.3136  0.2233\n",
            "   0.3135]\n",
            " [-0.5618  0.0714 -0.2775  0.34   -0.2975 -0.0228  0.2278 -0.6466  0.1851\n",
            "   0.1175]\n",
            " [ 0.3846 -0.1021 -0.2566 -0.7097  0.3402 -0.4673 -0.001   0.3643 -0.204\n",
            "   0.2853]\n",
            " [ 0.0204 -0.4202  0.252  -0.3648 -0.0984 -0.4568  0.3257  0.2878  0.28\n",
            "   0.045 ]]\n",
            "  üéØ Biases (shape (10,)):\n",
            "[-0.6329  0.2947  0.2195 -0.0843  0.1345  0.1397  0.0834  0.565  -1.\n",
            "  0.5257]\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(precision=4, suppress=True)  # nicer float formatting\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    print(f\"\\n Layer {i} - {layer.name}\")\n",
        "\n",
        "    if weights:\n",
        "        W, b = weights\n",
        "\n",
        "        print(f\"   Weights (shape {W.shape}):\\n{W}\")\n",
        "        print(f\"   Biases (shape {b.shape}):\\n{b}\")\n",
        "    else:\n",
        "        print(\"  No weights or biases (probably Flatten or non-trainable)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint and custom array creation"
      ],
      "metadata": {
        "id": "PA0GSEFgoIGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8sOe7xuSDpC",
        "outputId": "2fe009b9-6aa1-4255-f4a0-62d5008b0f04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "\n",
            "üì¶ weights_3d[0] shape: (784, 64)\n",
            "[[-0.0404  0.0175  0.0349 ... -0.0367  0.0332  0.0368]\n",
            " [ 0.0502  0.006   0.053  ...  0.0431 -0.0429  0.0043]\n",
            " [-0.0357 -0.0631 -0.0229 ... -0.0581  0.0068 -0.0099]\n",
            " ...\n",
            " [ 0.0349  0.0302  0.0576 ... -0.0651 -0.0621  0.0737]\n",
            " [-0.0685  0.0301 -0.0584 ...  0.0332 -0.0072  0.0522]\n",
            " [ 0.0221 -0.0258 -0.0322 ...  0.0641 -0.021   0.0266]]\n",
            "\n",
            "üì¶ weights_3d[1] shape: (64, 10)\n",
            "[[-0.7513 -0.551  -0.5196 -0.2909  0.3418  0.5205 -0.6841  0.3352  0.0804\n",
            "   0.0871]\n",
            " [-0.3494 -0.4796  0.1148  0.2103 -0.3412 -0.1329  0.1632  0.0236 -0.0909\n",
            "   0.5239]\n",
            " [ 0.2174 -0.5927 -0.3625  0.3627  0.3058 -0.2927 -0.5054  0.3029 -0.3606\n",
            "   0.3587]\n",
            " [-0.6157  0.4076  0.2998  0.2292  0.4464 -0.1854 -0.5913  0.3386 -0.066\n",
            "  -0.2397]\n",
            " [-0.2541  0.3129  0.312   0.5335  0.0227 -0.3012 -0.7476  0.4464 -0.5422\n",
            "  -0.2736]\n",
            " [ 0.3121  0.1529  0.2676  0.4364 -0.5823 -0.3679 -0.3048  0.0852  0.3537\n",
            "  -0.3922]\n",
            " [-0.4397 -0.0741 -0.034   0.1825  0.3738 -0.7583 -0.2857 -0.3135  0.5854\n",
            "   0.371 ]\n",
            " [-0.8875  0.5651  0.526  -0.5867 -0.3041  0.5102  0.0692 -0.1315 -0.2397\n",
            "  -0.5571]\n",
            " [-0.5532 -0.4288  0.4968  0.3241  0.2877 -0.3575  0.2219  0.3952 -0.1872\n",
            "   0.256 ]\n",
            " [ 0.3162  0.0235  0.1128  0.3253 -0.6417  0.4933 -0.3536 -0.3961 -0.9576\n",
            "   0.2146]\n",
            " [-0.472   0.4685 -0.0022  0.2693 -0.5435 -0.1573 -0.7541  0.1716  0.1742\n",
            "   0.6197]\n",
            " [ 0.3485  0.138  -0.2464  0.3224 -0.5378  0.188  -0.6151  0.1503 -0.2141\n",
            "   0.5713]\n",
            " [ 0.5747  0.1508  0.4747 -0.096   0.04   -0.1617 -0.2231  0.3879 -1.\n",
            "  -0.3793]\n",
            " [-0.3031  0.1725  0.1334  0.3719 -0.2747 -0.7971 -0.7468  0.2518 -0.1922\n",
            "  -0.1663]\n",
            " [ 0.4392 -0.0619  0.4748 -0.6129 -0.5961  0.4489  0.0743  0.3788 -0.1588\n",
            "  -0.5652]\n",
            " [-0.4269  0.3156  0.1885  0.3347 -0.431  -0.3227  0.4512  0.3188  0.0969\n",
            "  -0.5411]\n",
            " [-0.3397 -0.2802  0.4578  0.0857  0.3755  0.0556  0.3365 -0.6607 -0.7007\n",
            "   0.4176]\n",
            " [ 0.3356 -0.5609 -0.3911 -0.5194  0.0136  0.5121  0.3341 -0.1038  0.0024\n",
            "   0.3772]\n",
            " [-0.0193  0.3121  0.6053 -0.3308 -0.1322 -0.0906  0.0198  0.4673 -0.6136\n",
            "  -0.2331]\n",
            " [-0.8153  0.2337  0.2634  0.2232  0.0839  0.2769  0.4634 -0.6601 -0.069\n",
            "  -0.1977]\n",
            " [-0.292   0.2242  0.4372 -0.46    0.5209 -0.2951  0.2503 -0.6705 -0.0306\n",
            "  -0.0166]\n",
            " [ 0.1011  0.4119 -0.4223  0.2019 -0.6024  0.0492  0.2106 -0.7725  0.1594\n",
            "  -0.3825]\n",
            " [-0.1948  0.2127  0.1818  0.4474  0.2933 -0.3269  0.0858 -0.4628 -0.3377\n",
            "   0.2882]\n",
            " [ 0.0941 -0.5442  0.3584 -0.162  -0.0751  0.4163  0.1344  0.2395  0.2136\n",
            "  -0.4807]\n",
            " [-0.2305  0.4809 -0.1806 -0.286  -0.3356 -0.3822  0.5011  0.018  -0.3017\n",
            "   0.2605]\n",
            " [ 0.1278  0.3031 -0.4312 -0.0338 -0.3033  0.0528 -0.3234 -0.5451  0.3303\n",
            "   0.5123]\n",
            " [-0.1922 -0.4004 -0.5515 -0.0954  0.5399  0.3589  0.408   0.2565  0.2194\n",
            "  -0.5506]\n",
            " [-0.6283 -0.2333  0.3449  0.3843 -0.7966  0.343   0.3128  0.3165 -0.5882\n",
            "  -0.1678]\n",
            " [-0.6556 -0.2619 -0.1852  0.2625 -0.0733  0.4614 -0.5516  0.3986  0.3733\n",
            "   0.0431]\n",
            " [ 0.0665 -0.4424  0.3352 -0.2335 -0.445   0.2661 -0.3393 -0.3235  0.3453\n",
            "   0.2909]\n",
            " [ 0.3761 -0.1408  0.3012  0.357   0.2969 -0.4704  0.3302 -0.4125  0.0574\n",
            "  -0.2645]\n",
            " [-0.069  -0.1552  0.6629 -0.4116 -0.3779 -0.3493  0.1742 -0.475   0.1035\n",
            "   0.0145]\n",
            " [-0.4991  0.3783 -0.3889  0.3506  0.1129  0.1453  0.1802 -0.3307 -0.8078\n",
            "   0.4456]\n",
            " [ 0.2069 -0.1163 -0.8291 -0.0546  0.2106 -0.0355  0.4267  0.2795 -0.4913\n",
            "   0.2803]\n",
            " [-0.4709  0.2273 -0.0515 -0.4094 -0.0682 -0.731  -0.4036  0.3434  0.3794\n",
            "   0.3641]\n",
            " [ 0.2666  0.143  -0.5994 -0.0479 -0.4746  0.6056  0.2836  0.4873 -0.8874\n",
            "  -0.4392]\n",
            " [ 0.1174  0.3798  0.0719 -0.683  -0.2718 -0.345   0.5692  0.5771 -0.1688\n",
            "  -0.2587]\n",
            " [-0.3819  0.4623 -0.9978 -0.8852 -0.1297  0.5353  0.6243 -0.5484 -0.2724\n",
            "   0.5037]\n",
            " [-0.463   0.3952 -0.206   0.3792  0.3086  0.3635 -0.3868  0.3957 -0.2147\n",
            "  -0.3518]\n",
            " [-0.4083 -0.2286  0.2938 -0.5697  0.4641 -0.4399 -0.171  -0.2113  0.1029\n",
            "   0.1425]\n",
            " [ 0.4992  0.0337  0.2102  0.2855 -0.4539 -0.0453 -0.6336  0.3603  0.1234\n",
            "  -0.5524]\n",
            " [-0.2004  0.2493  0.1278 -0.2269  0.6182  0.6034 -0.7028 -0.0021 -0.6455\n",
            "  -0.3585]\n",
            " [-0.2925  0.1647  0.1224 -0.3637  0.1449  0.306   0.2722 -0.3825  0.3996\n",
            "  -0.6126]\n",
            " [ 0.3171  0.3964  0.1692 -0.078  -0.5203 -0.0074  0.3788 -0.6495  0.1976\n",
            "  -0.5289]\n",
            " [-0.5395 -0.3497 -0.586  -0.3759  0.376   0.4849  0.2513  0.0902  0.2433\n",
            "   0.1739]\n",
            " [-0.0641  0.3602  0.1049 -0.5246  0.3264  0.0899 -0.4536  0.1531 -0.9568\n",
            "   0.4298]\n",
            " [ 0.2939 -0.1305 -0.2869 -0.1967  0.1488 -0.4053  0.2394  0.3384 -0.5375\n",
            "   0.4493]\n",
            " [-0.7151  0.1247 -0.6951  0.2673  0.1465  0.3909 -0.2387 -0.7153  0.2873\n",
            "   0.516 ]\n",
            " [ 0.3405  0.5384 -0.3838 -0.0077  0.1587 -0.3621  0.5067 -0.1002 -0.5172\n",
            "  -0.4317]\n",
            " [ 0.3455 -0.4143 -0.4362  0.319   0.1067  0.2806 -0.4623  0.1559  0.0667\n",
            "   0.2597]\n",
            " [ 0.1728 -0.1292  0.1009  0.1664 -0.8979  0.1603 -0.5068  0.1721  0.3328\n",
            "  -0.091 ]\n",
            " [ 0.0529  0.2032  0.2363 -0.105   0.4815  0.5412 -0.114  -0.4998 -0.3438\n",
            "  -0.5722]\n",
            " [ 0.1753 -0.4946 -0.2119  0.1705  0.3523  0.2867 -0.2422 -0.5387  0.1477\n",
            "   0.1172]\n",
            " [ 0.3941 -0.3146 -0.3727  0.1199  0.3393  0.2858  0.0365  0.4781 -0.5685\n",
            "  -0.635 ]\n",
            " [ 0.3207 -0.4589 -0.2909 -0.3568  0.1897 -0.5533  0.2851  0.3153  0.4648\n",
            "   0.1143]\n",
            " [-0.2142  0.347   0.2384 -0.0003 -0.5892  0.423  -0.9887  0.4999 -0.4691\n",
            "   0.4175]\n",
            " [ 0.2386  0.2625  0.1792 -0.4189 -0.0865  0.4697  0.0613 -0.6164  0.2386\n",
            "  -0.5297]\n",
            " [-0.2252  0.1246 -0.1631  0.5286 -0.4041 -0.1008  0.1368 -0.4073  0.295\n",
            "  -0.754 ]\n",
            " [ 0.2095 -0.5861 -0.2961 -0.09    0.2021 -0.2473 -0.7122  0.2241  0.373\n",
            "   0.2527]\n",
            " [ 0.4295 -0.2942  0.3298 -0.6105  0.1512 -0.5471  0.1198 -0.2694 -0.1533\n",
            "   0.4187]\n",
            " [-0.3811  0.4943 -0.4065 -0.3159  0.2062  0.2506  0.4453  0.6501 -0.4596\n",
            "  -0.6056]\n",
            " [-0.6507  0.1479 -0.6562  0.1991  0.1791  0.0571 -0.4261  0.1449  0.3876\n",
            "  -0.0993]\n",
            " [ 0.3779 -0.1685  0.0883 -0.4395  0.225  -0.6903  0.3863  0.4014  0.4517\n",
            "   0.1575]\n",
            " [ 0.1401 -0.6007  0.3346  0.3417 -0.6045  0.402   0.2664 -0.0339  0.3027\n",
            "  -0.4593]]\n"
          ]
        }
      ],
      "source": [
        "weight_matrices = []\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if weights:\n",
        "        W, _ = weights\n",
        "        weight_matrices.append(W)\n",
        "\n",
        "weights_3d = np.array(weight_matrices, dtype=object)\n",
        "print(weights_3d.shape)\n",
        "for i, w in enumerate(weights_3d):\n",
        "    print(f\"\\nüì¶ weights_3d[{i}] shape: {w.shape}\")\n",
        "    print(w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding empty places with 0s to fit the RECON Architecture and making uniform array sizes, irrespective of neurons utilized"
      ],
      "metadata": {
        "id": "rWmKjI9LoVl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i96u2bn-QLdO",
        "outputId": "fa1bf5ea-b2b0-4d65-fca8-d48b777daaa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "\n",
            "üì¶ weights_3d[0] shape: (784, 64)\n",
            "n=784, m=64\n",
            "\n",
            "üì¶ weights_3d[1] shape: (64, 10)\n",
            "n=64, m=10\n"
          ]
        }
      ],
      "source": [
        "#let n be the number of neurons in this layer and m be the number of neurons in the next layer\n",
        "n_layers=weights_3d.shape[0]\n",
        "print(n_layers)\n",
        "iterative_sequence=[]\n",
        "for i, w in enumerate(weights_3d):\n",
        "    print(f\"\\nüì¶ weights_3d[{i}] shape: {w.shape}\")\n",
        "    #print(w)\n",
        "    current_layer=weights_3d[i]\n",
        "    n=current_layer.shape[0]\n",
        "    m=current_layer.shape[1]\n",
        "    if i==0:\n",
        "        for j in range(784): #no of rows in w matrix\n",
        "          for k in range(64): #no of cols in w matrix\n",
        "            iterative_sequence.append(current_layer[j][k]) #for first layer, the shape is neck to neck, so not padding 0s right now.\n",
        "\n",
        "    else:\n",
        "        for j in range(64): #row index\n",
        "          if (j<n):\n",
        "            for k in range(64): #column index\n",
        "              if(k<m):\n",
        "                iterative_sequence.append(current_layer[j][k])\n",
        "              elif(k>=m):\n",
        "                  iterative_sequence.append(0)\n",
        "          elif (j>=n):\n",
        "            for k in range(64):\n",
        "              iterative_sequence.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"n={n}, m={m}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2lrV8Chg3cE"
      },
      "source": [
        "Checkpoint : Trial on 4x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98WDSGh2g125",
        "outputId": "23912b0d-491d-4df9-ce17-3a9458bd9952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 3 5]\n",
            " [2 4 6]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ws=np.array([(1,3,5) , (2,4,6)])\n",
        "print(ws)\n",
        "ws.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ-X2MZWha5C",
        "outputId": "be4fd784-a43b-445b-89a5-ed4f2ae46351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 3 5 0 2 4 6 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "test_seq=[]\n",
        "n=ws.shape[0]\n",
        "m=ws.shape[1]\n",
        "for j in range(4): #row index\n",
        "  if (j<n):\n",
        "     for k in range(4): #column index\n",
        "        if(k<m):\n",
        "            test_seq.append(ws[j][k])\n",
        "        elif(k>=m):\n",
        "            test_seq.append(0)\n",
        "  elif (j>=n):\n",
        "      for k in range(4):\n",
        "              test_seq.append(0)\n",
        "test_seq=np.array(test_seq)\n",
        "print(test_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXiXN_ZcjZMI"
      },
      "source": [
        "Mem File Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhRsoRX0jYu1"
      },
      "outputs": [],
      "source": [
        "# Ensure the .mem file exists and is empty before starting\n",
        "with open(\"/content/fixed_point_random.mem\", \"w\") as file:\n",
        "    pass  # This creates the file if it doesn't exist, and clears it if it does\n",
        "\n",
        "# Start collecting user inputs and writing fixed-point binary\n",
        "i=0\n",
        "while (i<len(iterative_sequence)):\n",
        "    num = float(iterative_sequence[i])\n",
        "    binary = num * 1024\n",
        "    a = int(binary)\n",
        "\n",
        "    if num >= 0:\n",
        "        binary_16bit = format(a, '016b')\n",
        "    else:\n",
        "        binary_16bit = format((1 << 16) + a, '016b')  # Two's complement for negative numbers\n",
        "\n",
        "    with open(\"/content/fixed_point_random.mem\", \"a\") as file:\n",
        "        file.write(binary_16bit + \"\\n\")\n",
        "    i=i+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_GvTl9KZpeyN",
        "outputId": "99f5a191-9ee4-4fa2-f4ba-bc657a9bacbd"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_1667dedc-7833-475b-a337-e4cfc1405d73\", \"fixed_point_random.mem\", 922771)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Path to your .mem file\n",
        "filepath = \"/content/fixed_point_random.mem\"\n",
        "\n",
        "# Trigger download\n",
        "files.download(filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Following code was created for formatting the files downloaded, has no link with logic, only the formatting to download the mem files and text files to be put in Memory and validation is performed in upcoming stages"
      ],
      "metadata": {
        "id": "SPLH5hjQop4C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7OBsdzx0Ybs"
      },
      "source": [
        "for bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je83vUya0Zbh",
        "outputId": "e1a1a5f8-aaec-42d0-9a82-2ba1ad0724fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74\n"
          ]
        }
      ],
      "source": [
        "bias_array=[]\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()\n",
        "    if weights:\n",
        "        _, b = weights  # We only use biases, not weights\n",
        "        bias_array.extend(b.tolist())\n",
        "print(len(bias_array))\n",
        "while (len(bias_array)<128)  :\n",
        "  bias_array.append(0)\n",
        "# Ensure the .mem file exists and is empty before starting\n",
        "with open(\"/content/bias.mem\", \"w\") as file:\n",
        "    pass  # This creates the file if it doesn't exist, and clears it if it does\n",
        "\n",
        "# Start collecting user inputs and writing fixed-point binary\n",
        "i=0\n",
        "while (i<len(bias_array)):\n",
        "    num = float(bias_array[i])\n",
        "    binary = num * 1024\n",
        "    a = int(binary)\n",
        "\n",
        "    if num >= 0:\n",
        "        binary_16bit = format(a, '016b')\n",
        "    else:\n",
        "        binary_16bit = format((1 << 16) + a, '016b')  # Two's complement for negative numbers\n",
        "\n",
        "    with open(\"/content/bias.mem\", \"a\") as file:\n",
        "        file.write(binary_16bit + \"\\n\")\n",
        "    i=i+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riVr0n0z0M3f"
      },
      "outputs": [],
      "source": [
        "# Open the original file in read mode and a new file to write the modified content\n",
        "with open('/content/sigmoid_bias.txt', 'r') as infile, open('/content/weights_sigmoid_mnist.txt', 'w') as outfile:\n",
        "    for line in infile:\n",
        "        line = line.strip()  # remove any newline or trailing whitespace\n",
        "        if line:  # skip empty lines\n",
        "            outfile.write(line + ',\\n')  # add comma and newline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puz6AVvNEwos"
      },
      "source": [
        "First Test Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "c0E3h7Ffqwcy",
        "outputId": "18a8bc7e-62ca-4751-b5fc-1dfa7d1569a2"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "can only convert an array of size 1 to a Python scalar",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8b6d6f023144>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìç Found label==1 at index:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlabel_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
          ]
        }
      ],
      "source": [
        "for i, (image, label) in enumerate(ds_test):\n",
        "    if label.numpy().item()== 1:\n",
        "        print(\"üìç Found label==1 at index:\", i)\n",
        "        image_np = image.numpy()\n",
        "        label_true = label.numpy()\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "xKaqkJLTEwVY",
        "outputId": "90e32cff-681e-4c34-ca05-b0a214844f5f"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mCannot take the length of shape with unknown rank.\u001b[0m\n\nArguments received by Sequential.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  ‚Ä¢ training=False\n  ‚Ä¢ mask=None",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6cf98ed9ef8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Run model prediction (add batch dimension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (1, 28, 28)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mCannot take the length of shape with unknown rank.\u001b[0m\n\nArguments received by Sequential.call():\n  ‚Ä¢ inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n  ‚Ä¢ training=False\n  ‚Ä¢ mask=None"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "# Grab a batch of test data (just 1 sample)\n",
        "for c in range(100):\n",
        "  for image, label in ds_test.take(1):\n",
        "    if label[c].numpy()==1:\n",
        "      image_np = image[c].numpy()  # shape: (28, 28)\n",
        "      label_true = label[c].numpy()\n",
        "      break\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "# Flatten and normalize the image (should already be normalized, but to be safe)\n",
        "input_array = image_np.flatten()  # shape: (784,)\n",
        "input_array = input_array.astype(np.float32)\n",
        "\n",
        "# Run model prediction (add batch dimension)\n",
        "input_batch = np.expand_dims(image_np, axis=0)  # shape: (1, 28, 28)\n",
        "logits = model.predict(input_batch)\n",
        "predicted_label = np.argmax(logits)\n",
        "\n",
        "# Print details\n",
        "print(\"üß† 784-Element Flattened Input Array (First 20 shown):\")\n",
        "print(input_array[:100])  # show only first 20 to keep it clean\n",
        "print(\"\\n‚úÖ Predicted Label:\", predicted_label)\n",
        "print(\"üéØ Actual Label:   \", label_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0yoGW2EF3DN",
        "outputId": "fa4e6bc8-562c-4570-ce63-db33267ca3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 binary values:\n",
            "['0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000', '0000000000000000']\n"
          ]
        }
      ],
      "source": [
        "binary_array = []\n",
        "i = 0\n",
        "\n",
        "while i < len(input_array):\n",
        "    num = float(input_array[i])\n",
        "    binary = num * 1024  # Fixed-point scaling\n",
        "    a = int(binary)\n",
        "\n",
        "    if num >= 0:\n",
        "        binary_16bit = format(a, '016b')\n",
        "    else:\n",
        "        binary_16bit = format((1 << 16) + a, '016b')  # Two's complement for negative values\n",
        "\n",
        "    binary_array.append(binary_16bit)\n",
        "    i += 1\n",
        "\n",
        "# Optional: print first few to confirm\n",
        "print(\"First 10 binary values:\")\n",
        "print(binary_array[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgwu_LGlGk8H"
      },
      "outputs": [],
      "source": [
        "# Check for any entries longer than 16 bits\n",
        "for idx, val in enumerate(binary_array):\n",
        "    if len(val) == 17:\n",
        "        print(f\"‚ö†Ô∏è  Index {idx} has a value with {len(val)} bits: {val}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3_8foXjs3FL"
      },
      "source": [
        "inputs[0] = 16'b0000000000000000;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2elEtXrHL28",
        "outputId": "ae51b8b7-8e8d-405b-8aa6-466071391c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Binary array successfully written to 'binary_output.txt'\n"
          ]
        }
      ],
      "source": [
        "with open(\"binary_output.txt\", \"w\") as file:\n",
        "    for binary_val in binary_array:\n",
        "        file.write(binary_val+\",\" + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Binary array successfully written to 'binary_output.txt'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGCUUpV0s8eq",
        "outputId": "062927a3-18f7-40d1-f2af-7445668c8ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Binary array successfully written to 'binary_output.txt'\n"
          ]
        }
      ],
      "source": [
        "with open(\"new_output.txt\", \"w\") as file:\n",
        "    for idx,binary_val in enumerate(binary_array):\n",
        "        file.write(\"inputs[\" + str(idx) + \"]= 16'b\"+ binary_val+\";\" + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Binary array successfully written to 'binary_output.txt'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOdtwHLPRC79",
        "outputId": "bd57d57d-2fac-444f-f3e7-c3dd502a620e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Changes made at the following indexes (line, index):\n",
            "[(288, 0), (1267, 0), (1295, 0), (1495, 0), (1722, 0), (1991, 0), (2110, 0), (2157, 0), (2395, 0), (2437, 0), (2931, 0), (3362, 0), (3457, 0), (4024, 0), (4203, 0), (4677, 0), (5004, 0), (5171, 0), (5190, 0), (5584, 0), (5617, 0), (6990, 0), (7059, 0), (7562, 0), (7654, 0), (7691, 0), (7983, 0), (8145, 0), (8522, 0), (8676, 0), (8975, 0), (8986, 0), (9021, 0), (9403, 0), (9434, 0), (10142, 0), (10574, 0), (10591, 0), (10705, 0), (10903, 0), (11077, 0), (11223, 0), (11629, 0), (11790, 0), (12021, 0), (12582, 0), (13292, 0), (13410, 0), (13887, 0), (14641, 0), (15169, 0), (15197, 0), (16163, 0), (16336, 0), (17296, 0), (18325, 0), (18552, 0), (18984, 0), (19256, 0), (19322, 0), (19882, 0), (20262, 0), (21199, 0), (21492, 0), (21958, 0), (22071, 0), (22394, 0), (22729, 0), (23352, 0), (23530, 0), (23646, 0), (23882, 0), (24132, 0), (24142, 0), (24490, 0), (24877, 0), (25890, 0), (26147, 0), (26594, 0), (26612, 0), (27145, 0), (27301, 0), (27304, 0), (27397, 0), (28059, 0), (28138, 0), (28269, 0), (28640, 0), (28745, 0), (28754, 0), (28924, 0), (30107, 0), (30295, 0), (30471, 0), (30935, 0), (31101, 0), (31229, 0), (31242, 0), (33027, 0), (34253, 0), (34434, 0), (34724, 0), (34905, 0), (35228, 0), (35272, 0), (35866, 0), (35984, 0), (36204, 0), (37575, 0), (39209, 0), (39416, 0), (39947, 0), (40030, 0), (40314, 0), (40572, 0), (41066, 0), (41453, 0), (41527, 0), (41809, 0), (42204, 0), (42615, 0), (42619, 0), (43700, 0), (43790, 0), (44049, 0), (44112, 0), (44139, 0), (44757, 0), (45053, 0), (45871, 0), (45926, 0), (46149, 0), (47084, 0), (47656, 0), (48106, 0), (48240, 0), (48563, 0), (48731, 0), (48916, 0), (49075, 0), (49331, 0), (49536, 0), (49830, 0), (49876, 0), (53256, 0)]\n"
          ]
        }
      ],
      "source": [
        "# Read the file and process the contents\n",
        "file_path = 'weights_sigmoid_mnist_comma.txt'  # Update the file path if needed\n",
        "\n",
        "# Initialize an empty list to store the indexes where changes are made\n",
        "changed_indexes = []\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Iterate through each line and each value\n",
        "for line_num, line in enumerate(lines):\n",
        "    values = line.strip().split(',')  # Assuming the values are comma-separated\n",
        "\n",
        "    for idx, value in enumerate(values):\n",
        "        if value == '10000000000000000':  # Check for the specific value\n",
        "            values[idx] = '0000000000000000'  # Replace with the new value\n",
        "            changed_indexes.append((line_num, idx))  # Store the index of the change\n",
        "\n",
        "    # Join the values back into a string and update the line\n",
        "    lines[line_num] = ','.join(values) + '\\n'\n",
        "\n",
        "# Save the modified file as 'weights_sigmoid_mnist_comma_updated.txt'\n",
        "output_file_path = 'weights_sigmoid_mnist_comma_updated.txt'\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.writelines(lines)\n",
        "\n",
        "# Return the indexes where changes were made\n",
        "print(\"Changes made at the following indexes (line, index):\")\n",
        "print(changed_indexes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qlOQ-14zYec"
      },
      "source": [
        "Test on 16 bit precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy-mCOjbzZ_O"
      },
      "outputs": [],
      "source": [
        "def binary16_to_float(binary_str):\n",
        "    assert len(binary_str) == 16, \"Binary string must be 16 bits\"\n",
        "\n",
        "    sign = -1 if binary_str[0] == '1' else 1\n",
        "    integer_part = int(binary_str[1:6], 2)\n",
        "    fractional_part = int(binary_str[6:], 2) / (2 ** 10)\n",
        "\n",
        "    return sign * (integer_part + fractional_part)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPECoj6OzczL"
      },
      "outputs": [],
      "source": [
        "decimal_array = []\n",
        "with open(\"input_binary.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        if line.strip():  # skip empty lines\n",
        "            decimal_array.append(binary16_to_float(line.strip()))\n",
        "\n",
        "input_array = np.array(decimal_array, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQq-1hC40gTL",
        "outputId": "a0a8bc32-3a7b-472a-dd36-07a3b3d6072f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.2451171875\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "\n",
            "üìä Final Layer Output (10 raw values):\n",
            "[-1.307642    1.6424227  -0.22620583 -3.6669445   1.4355907   1.8929176\n",
            " -0.9432297   1.6240625  -3.6339684  -1.2661057 ]\n",
            "\n",
            "‚úÖ Predicted Label: 5\n",
            "üß† Model Output: [[-1.307642    1.6424227  -0.22620583 -3.6669445   1.4355907   1.8929176\n",
            "  -0.9432297   1.6240625  -3.6339684  -1.2661057 ]]\n",
            "üéØ Predicted Label: 5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def binary16_to_float(binary_str):\n",
        "    binary_str = binary_str.strip()\n",
        "\n",
        "    if len(binary_str) != 16:\n",
        "        binary_str = \"0000000000000000\"\n",
        "\n",
        "    # Convert full 16-bit binary to signed integer (handling two's complement)\n",
        "    int_val = int(binary_str, 2)\n",
        "    if int_val >= 2**15:  # Check if negative in 2's complement\n",
        "        int_val -= 2**16\n",
        "\n",
        "    # Convert back to float using fixed-point scaling\n",
        "    return int_val / 1024.0\n",
        "\n",
        "num=binary16_to_float(str(1111111100000101)) #-0.245117\n",
        "print(num)\n",
        "# üìÇ Load input\n",
        "with open(\"/content/9_num.txt\", \"r\") as f:\n",
        "    input_vals = [binary16_to_float(line) for line in f if line.strip()]\n",
        "input_array1=np.array(input_vals, dtype=np.float32)\n",
        "input_array = np.array(input_vals, dtype=np.float32).reshape(1, 28, 28)\n",
        "\n",
        "# üìÇ Load weights (flattened with padding)\n",
        "with open(\"/content/sigmoid_weights.txt\", \"r\") as f:\n",
        "    weights_vals = [binary16_to_float(line) for line in f if line.strip()]\n",
        "weights_vals = np.array(weights_vals, dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "# üîÅ Reshape back from flat list\n",
        "# Layer 1: 784√ó64 = 50176\n",
        "# Layer 2: 64√ó10 = 640 (from 64√ó64 padded block)\n",
        "\n",
        "w1_flat = weights_vals[:784 * 64]\n",
        "w2_flat = weights_vals[784 * 64 : 784 * 64 + 64 * 64]  # second block\n",
        "\n",
        "w1 = w1_flat.reshape((784, 64))\n",
        "w2_full = w2_flat.reshape((64, 64))\n",
        "w2 = w2_full[:, :10]  # trim to actual number of outputs\n",
        "\n",
        "# üìÇ Load biases\n",
        "with open(\"/content/sigmoid_bias.txt\", \"r\") as f:\n",
        "    bias_vals = [binary16_to_float(line) for line in f if line.strip()]\n",
        "bias_vals = np.array(bias_vals, dtype=np.float32)\n",
        "\n",
        "b1 = bias_vals[:64]\n",
        "b2 = bias_vals[64:74]  # next 10\n",
        "\n",
        "#checker\n",
        "for weights in weights_vals:\n",
        "  if weights>1 or weights<-1:\n",
        "    print(\"oob detected in weights\")\n",
        "for bias in bias_vals:\n",
        "  if bias>1 or bias<-1:\n",
        "    print(\"oob detected in bias\")\n",
        "for ips in input_array1:\n",
        "  if ips>1 or ips<-1:\n",
        "    print(\"oob detected in ips\")\n",
        "\n",
        "# üß† Define the model structure\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "#model.build(input_shape=(1, 28, 28))\n",
        "\n",
        "# üß© Set model weights\n",
        "model.layers[1].set_weights([w1, b1])\n",
        "model.layers[2].set_weights([w2, b2])\n",
        "\n",
        "# üöÄ Run prediction\n",
        "output = model.predict(input_array)\n",
        "\n",
        "# üß† Show full output of the last Dense layer (logits or softmax inputs)\n",
        "print(\"\\nüìä Final Layer Output (10 raw values):\")\n",
        "print(output[0])  # [0] because batch size = 1\n",
        "\n",
        "# üéØ Predicted class\n",
        "predicted_label = np.argmax(output)\n",
        "print(\"\\n‚úÖ Predicted Label:\", predicted_label)\n",
        "predicted_label = np.argmax(output)\n",
        "\n",
        "print(\"üß† Model Output:\", output)\n",
        "print(\"üéØ Predicted Label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint: Running inference on binary obtained results and weights and biases to valdiate the calculations/results performed by CORDIC"
      ],
      "metadata": {
        "id": "DGBmZroapkOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X4RzJuq6f3f",
        "outputId": "c37adee3-d942-48f4-e50e-4e198d6d5d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.2451171875\n",
            "\n",
            "üìä Final Layer Output (10 raw values):\n",
            "tf.Tensor(\n",
            "[0.17790206 0.08674706 0.9901589  0.46039918 0.1365747  0.4050762\n",
            " 0.16085711 0.11029727 0.3611843  0.04754891], shape=(10,), dtype=float32)\n",
            "\n",
            "‚úÖ Predicted Label: 2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def binary16_to_float(binary_str):\n",
        "    binary_str = binary_str.strip()\n",
        "\n",
        "    if len(binary_str) != 16:\n",
        "        binary_str = \"0000000000000000\"\n",
        "\n",
        "    # Convert full 16-bit binary to signed integer (handling two's complement)\n",
        "    int_val = int(binary_str, 2)\n",
        "    if int_val >= 2**15:  # Check if negative in 2's complement\n",
        "        int_val -= 2**16\n",
        "\n",
        "    # Convert back to float using fixed-point scaling\n",
        "    return int_val / 1024.0\n",
        "\n",
        "num = binary16_to_float(str(1111111100000101)) #-0.245117\n",
        "print(num)\n",
        "\n",
        "# üìÇ Load input\n",
        "with open(\"/content/input_binary_2.txt\", \"r\") as f:\n",
        "    input_vals = [binary16_to_float(line) for line in f if line.strip()]\n",
        "input_array1 = np.array(input_vals, dtype=np.float32)\n",
        "input_array = np.array(input_vals, dtype=np.float32).reshape(1, 28, 28)\n",
        "\n",
        "# üìÇ Load weights (flattened with padding)\n",
        "with open(\"/content/sigmoid_weights.txt\", \"r\") as f:\n",
        "    weights_vals = [binary16_to_float(line) for line in f if line.strip()]\n",
        "weights_vals = np.array(weights_vals, dtype=np.float32)\n",
        "\n",
        "# üîÅ Reshape back from flat list\n",
        "w1_flat = weights_vals[:784 * 64]\n",
        "w2_flat = weights_vals[784 * 64 : 784 * 64 + 64 * 64]  # second block\n",
        "\n",
        "w1 = w1_flat.reshape((784, 64))\n",
        "w2_full = w2_flat.reshape((64, 64))\n",
        "w2 = w2_full[:, :10]  # trim to actual number of outputs\n",
        "\n",
        "# üìÇ Load biases\n",
        "with open(\"/content/9_num.txt\", \"r\") as f:\n",
        "    bias_vals = [binary16_to_float(line) for line in f if line.strip()]\n",
        "bias_vals = np.array(bias_vals, dtype=np.float32)\n",
        "\n",
        "b1 = bias_vals[:64]\n",
        "b2 = bias_vals[64:74]  # next 10\n",
        "\n",
        "#checker\n",
        "for weights in weights_vals:\n",
        "  if weights > 1 or weights < -1:\n",
        "    print(\"oob detected in weights\")\n",
        "for bias in bias_vals:\n",
        "  if bias > 1 or bias < -1:\n",
        "    print(\"oob detected in bias\")\n",
        "for ips in input_array1:\n",
        "  if ips > 1 or ips < -1:\n",
        "    print(\"oob detected in ips\")\n",
        "\n",
        "# üß† Define the model structure\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# üß© Set model weights\n",
        "model.layers[1].set_weights([w1, b1])\n",
        "model.layers[2].set_weights([w2, b2])\n",
        "\n",
        "# Define a custom function to monitor layer outputs and check the value range\n",
        "def check_intermediate_values(layer_outputs, layer_name):\n",
        "    if np.any(layer_outputs > 1) or np.any(layer_outputs < -1):\n",
        "        print(f\"‚ö†Ô∏è Out-of-Bounds detected in {layer_name}!\")\n",
        "        print(f\"layer output {layer_outputs}\")\n",
        "        print(f\"{layer_name} output range: Min = {np.min(layer_outputs)}, Max = {np.max(layer_outputs)}\")\n",
        "\n",
        "# Add hooks for intermediate layer outputs\n",
        "# First, define a custom model with hooks\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, base_model):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Layer 1 (Flatten) output\n",
        "        x = self.base_model.layers[0](inputs)\n",
        "        check_intermediate_values(x.numpy(), \"Flatten Layer\")\n",
        "\n",
        "        # Layer 2 (Dense + ReLU) output\n",
        "        x = self.base_model.layers[1](x)\n",
        "        check_intermediate_values(x.numpy(), \"ReLU Layer\")\n",
        "\n",
        "        # Layer 3 (Dense) output\n",
        "        x = self.base_model.layers[2](x)\n",
        "        check_intermediate_values(x.numpy(), \"Output Layer\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create the model with the custom class\n",
        "custom_model = MyModel(model)\n",
        "\n",
        "# üöÄ Run prediction\n",
        "output = custom_model(input_array)\n",
        "\n",
        "# üß† Show full output of the last Dense layer (logits or softmax inputs)\n",
        "print(\"\\nüìä Final Layer Output (10 raw values):\")\n",
        "print(output[0])  # [0] because batch size = 1\n",
        "\n",
        "# üéØ Predicted class\n",
        "predicted_label = np.argmax(output)\n",
        "print(\"\\n‚úÖ Predicted Label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint: Running inference on Binary Obtained data and checking for overflows(value exceeding -1 to 1) in midway calculations"
      ],
      "metadata": {
        "id": "G_YA7AsKpUhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS_HP57UJB3p",
        "outputId": "91b240f6-be38-462a-d9ad-40a057a4663c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Flatten Layer: Checking 784 neurons\n",
            "‚úÖ Flatten Layer Min: 0.000000, Max: 0.000000\n",
            "\n",
            "üîç Dense Layer 1 (pre-Sigmoid: Checking 64 neurons\n",
            "‚úÖ Dense Layer 1 (pre-Sigmoid Min: -0.636719, Max: 1.000000\n",
            "\n",
            "üîç Dense Layer 1 (post-Sigmoid): Checking 64 neurons\n",
            "‚úÖ Dense Layer 1 (post-Sigmoid) Min: 0.000000, Max: 1.000000\n",
            "\n",
            "üîç Output Layer (Dense Layer 2): Checking 10 neurons\n",
            "‚ö†Ô∏è Neuron 0: OOB value = -1.307642\n",
            "‚ö†Ô∏è Neuron 1: OOB value = 1.642423\n",
            "‚ö†Ô∏è Neuron 3: OOB value = -3.666945\n",
            "‚ö†Ô∏è Neuron 4: OOB value = 1.435591\n",
            "‚ö†Ô∏è Neuron 5: OOB value = 1.892918\n",
            "‚ö†Ô∏è Neuron 7: OOB value = 1.624063\n",
            "‚ö†Ô∏è Neuron 8: OOB value = -3.633968\n",
            "‚ö†Ô∏è Neuron 9: OOB value = -1.266106\n",
            "‚úÖ Output Layer (Dense Layer 2) Min: -3.666945, Max: 1.892918\n",
            "\n",
            "‚úÖ Predicted Label: 5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def binary16_to_float(binary_str):\n",
        "    binary_str = binary_str.strip()\n",
        "    if len(binary_str) != 16:\n",
        "        binary_str = \"0000000000000000\"\n",
        "    int_val = int(binary_str, 2)\n",
        "    if int_val >= 2**15:\n",
        "        int_val -= 2**16\n",
        "    return int_val / 1024.0\n",
        "\n",
        "# Load inputs, weights, and biases (same as your code)...\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(64, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(10 , activation='sigmoid' )\n",
        "])\n",
        "model.layers[1].set_weights([w1, b1])\n",
        "model.layers[2].set_weights([w2, b2])\n",
        "\n",
        "# üëÅÔ∏è Detailed neuron-level monitoring\n",
        "def check_neuron_outputs(values, layer_name):\n",
        "    values_np = values.numpy().reshape(-1)  # convert to flat NumPy array\n",
        "    print(f\"\\nüîç {layer_name}: Checking {len(values_np)} neurons\")\n",
        "    for i, v in enumerate(values_np):\n",
        "        if v < -1 or v > 1:\n",
        "            print(f\"‚ö†Ô∏è Neuron {i}: OOB value = {v:.6f}\")\n",
        "    print(f\"‚úÖ {layer_name} Min: {np.min(values_np):.6f}, Max: {np.max(values_np):.6f}\")\n",
        "\n",
        "\n",
        "# Forward pass step by step with neuron output check\n",
        "flatten_output = model.layers[0](input_array)  # shape: (1, 784)\n",
        "dense1_linear = tf.matmul(flatten_output, w1) + b1  # before activation\n",
        "dense1_activated = tf.nn.relu(dense1_linear)\n",
        "\n",
        "dense2_output = tf.matmul(dense1_activated, w2) + b2\n",
        "\n",
        "# üß† Check each stage\n",
        "check_neuron_outputs(flatten_output, \"Flatten Layer\")\n",
        "check_neuron_outputs(dense1_linear, \"Dense Layer 1 (pre-Sigmoid\")\n",
        "check_neuron_outputs(dense1_activated, \"Dense Layer 1 (post-Sigmoid)\")\n",
        "check_neuron_outputs(dense2_output, \"Output Layer (Dense Layer 2)\")\n",
        "\n",
        "# üéØ Final Prediction\n",
        "predicted_label = np.argmax(dense2_output.numpy())\n",
        "print(\"\\n‚úÖ Predicted Label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoSUE7NUdYau",
        "outputId": "ce209481-7c77-4795-a4b9-7f8f7d3990e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "\n",
            "üì¶ weights_3d[0] shape: (784, 64)\n",
            "n=784, m=64\n",
            "\n",
            "üì¶ weights_3d[1] shape: (64, 10)\n",
            "n=64, m=10\n"
          ]
        }
      ],
      "source": [
        "#let n be the number of neurons in this layer and m be the number of neurons in the next layer\n",
        "n_layers=weights_3d.shape[0]\n",
        "print(n_layers)\n",
        "iterative_sequence=[]\n",
        "for i, w in enumerate(weights_3d):\n",
        "    print(f\"\\nüì¶ weights_3d[{i}] shape: {w.shape}\")\n",
        "    #print(w)\n",
        "    current_layer=weights_3d[i]\n",
        "    n=current_layer.shape[0]\n",
        "    m=current_layer.shape[1]\n",
        "    if i==0:\n",
        "\n",
        "      for j in range(1024): #row index\n",
        "          if (j<n):\n",
        "            for k in range(64): #column index\n",
        "              if(k<m):\n",
        "                iterative_sequence.append(current_layer[j][k])\n",
        "              elif(k>=m):\n",
        "                  iterative_sequence.append(0)\n",
        "          elif (j>=n):\n",
        "            for k in range(64):\n",
        "              iterative_sequence.append(0)\n",
        "    else:\n",
        "        for j in range(64): #row index\n",
        "          if (j<n):\n",
        "            for k in range(64): #column index\n",
        "              if(k<m):\n",
        "                iterative_sequence.append(current_layer[j][k])\n",
        "              elif(k>=m):\n",
        "                  iterative_sequence.append(0)\n",
        "          elif (j>=n):\n",
        "            for k in range(64):\n",
        "              iterative_sequence.append(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"n={n}, m={m}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTSjNkhCd9Wr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQKe0E2cb9jAZ/qUJ59Mzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}